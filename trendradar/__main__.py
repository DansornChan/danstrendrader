# coding=utf-8
"""
TrendRadar ä¸»ç¨‹åº

çƒ­ç‚¹æ–°é—»èšåˆä¸åˆ†æå·¥å…·
æ”¯æŒ: python -m trendradar
"""

import os
import webbrowser
import json
import requests
import logging  # [ä¿®å¤] å¼•å…¥ logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from difflib import SequenceMatcher

from trendradar.context import AppContext
from trendradar import __version__
from trendradar.core import load_config
from trendradar.core.analyzer import convert_keyword_stats_to_platform_stats
from trendradar.crawler import DataFetcher
from trendradar.storage import convert_crawl_results_to_news_data
from trendradar.utils.time import is_within_days
from trendradar.ai import AIAnalyzer, AIAnalysisResult

# [ä¿®å¤] åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def check_version_update(
    current_version: str, version_url: str, proxy_url: Optional[str] = None
) -> Tuple[bool, Optional[str]]:
    """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
    try:
        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/plain, */*",
            "Cache-Control": "no-cache",
        }

        response = requests.get(
            version_url, proxies=proxies, headers=headers, timeout=10
        )
        response.raise_for_status()

        remote_version = response.text.strip()
        print(f"å½“å‰ç‰ˆæœ¬: {current_version}, è¿œç¨‹ç‰ˆæœ¬: {remote_version}")

        # æ¯”è¾ƒç‰ˆæœ¬
        def parse_version(version_str):
            try:
                parts = version_str.strip().split(".")
                if len(parts) != 3:
                    raise ValueError("ç‰ˆæœ¬å·æ ¼å¼ä¸æ­£ç¡®")
                return int(parts[0]), int(parts[1]), int(parts[2])
            except:
                return 0, 0, 0

        current_tuple = parse_version(current_version)
        remote_tuple = parse_version(remote_version)

        need_update = current_tuple < remote_tuple
        return need_update, remote_version if need_update else None

    except Exception as e:
        print(f"ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
        return False, None


# === ä¸»åˆ†æå™¨ ===
class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""

    # æ¨¡å¼ç­–ç•¥å®šä¹‰
    MODE_STRATEGIES = {
        "incremental": {
            "mode_name": "å¢é‡æ¨¡å¼",
            "description": "å¢é‡æ¨¡å¼ï¼ˆåªå…³æ³¨æ–°å¢æ–°é—»ï¼Œæ— æ–°å¢æ—¶ä¸æ¨é€ï¼‰",
            "report_type": "å¢é‡åˆ†æ",
            "should_send_notification": True,
        },
        "current": {
            "mode_name": "å½“å‰æ¦œå•æ¨¡å¼",
            "description": "å½“å‰æ¦œå•æ¨¡å¼ï¼ˆå½“å‰æ¦œå•åŒ¹é…æ–°é—» + æ–°å¢æ–°é—»åŒºåŸŸ + æŒ‰æ—¶æ¨é€ï¼‰",
            "report_type": "å½“å‰æ¦œå•",
            "should_send_notification": True,
        },
        "daily": {
            "mode_name": "å…¨å¤©æ±‡æ€»æ¨¡å¼",
            "description": "å…¨å¤©æ±‡æ€»æ¨¡å¼ï¼ˆæ‰€æœ‰åŒ¹é…æ–°é—» + æ–°å¢æ–°é—»åŒºåŸŸ + æŒ‰æ—¶æ¨é€ï¼‰",
            "report_type": "å…¨å¤©æ±‡æ€»",
            "should_send_notification": True,
        },
    }

    def __init__(self):
        # åŠ è½½é…ç½®
        print("æ­£åœ¨åŠ è½½é…ç½®...")
        config = load_config()
        print(f"TrendRadar v{__version__} é…ç½®åŠ è½½å®Œæˆ")
        print(f"ç›‘æ§å¹³å°æ•°é‡: {len(config['PLATFORMS'])}")
        print(f"æ—¶åŒº: {config.get('TIMEZONE', 'Asia/Shanghai')}")

        # åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
        self.ctx = AppContext(config)

        self.request_interval = self.ctx.config["REQUEST_INTERVAL"]
        self.report_mode = self.ctx.config["REPORT_MODE"]
        self.rank_threshold = self.ctx.rank_threshold
        self.is_github_actions = os.environ.get("GITHUB_ACTIONS") == "true"
        self.is_docker_container = self._detect_docker_environment()
        self.update_info = None
        self.proxy_url = None
        self._setup_proxy()
        self.data_fetcher = DataFetcher(self.proxy_url)

        # åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨ï¼ˆä½¿ç”¨ AppContextï¼‰
        self._init_storage_manager()

        if self.is_github_actions:
            self._check_version_update()

    def _init_storage_manager(self) -> None:
        """åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨ï¼ˆä½¿ç”¨ AppContextï¼‰"""
        # è·å–æ•°æ®ä¿ç•™å¤©æ•°ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        env_retention = os.environ.get("STORAGE_RETENTION_DAYS", "").strip()
        if env_retention:
            # ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®
            self.ctx.config["STORAGE"]["RETENTION_DAYS"] = int(env_retention)

        self.storage_manager = self.ctx.get_storage_manager()
        print(f"å­˜å‚¨åç«¯: {self.storage_manager.backend_name}")

        retention_days = self.ctx.config.get("STORAGE", {}).get("RETENTION_DAYS", 0)
        if retention_days > 0:
            print(f"æ•°æ®ä¿ç•™å¤©æ•°: {retention_days} å¤©")

    def _detect_docker_environment(self) -> bool:
        """æ£€æµ‹æ˜¯å¦è¿è¡Œåœ¨ Docker å®¹å™¨ä¸­"""
        try:
            if os.environ.get("DOCKER_CONTAINER") == "true":
                return True

            if os.path.exists("/.dockerenv"):
                return True

            return False
        except Exception:
            return False

    def _should_open_browser(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰“å¼€æµè§ˆå™¨"""
        return not self.is_github_actions and not self.is_docker_container

    # --- ğŸ†• æ–°å¢ï¼šè·å–ç”¨æˆ·æŒä»“ä¸Šä¸‹æ–‡ ---
    def _fetch_portfolio_context(self) -> str:
        """
        ä» GitHub è·å–ç”¨æˆ·æŒä»“é…ç½®ï¼Œå¹¶ç”Ÿæˆ A è‚¡ä»£ç è¯†åˆ«ä¸Šä¸‹æ–‡
        """
        # è¿™é‡Œç¡¬ç¼–ç ä½ çš„ä»“åº“åœ°å€ï¼Œæˆ–è€…ä½ å¯ä»¥æŠŠå®ƒæ”¾åˆ° config.yaml é‡Œè¯»å–
        url = "https://raw.githubusercontent.com/DansornChan/daily_stock_analysis/main/portfolio.json"
        
        print("[Context] æ­£åœ¨åŒæ­¥æŒä»“æ•°æ®...")
        try:
            # å¦‚æœæ˜¯ç§æœ‰åº“ï¼Œå¯ä»¥åœ¨ headers ä¸­åŠ å…¥ Authorization
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                # å…¼å®¹å¤„ç†ï¼šå¦‚æœæ˜¯ list ç›´æ¥ç”¨ï¼Œå¦‚æœæ˜¯ dict å– keys
                if isinstance(data, dict):
                    codes = list(data.keys())
                elif isinstance(data, list):
                    codes = data
                else:
                    codes = []
                
                # è¿‡æ»¤å‡º 6 ä½æ•°å­—ä»£ç  (Aè‚¡ç‰¹å¾)
                a_share_codes = [str(c) for c in codes if str(c).isdigit() and len(str(c)) == 6]
                
                if not a_share_codes:
                    return ""
                
                # ç”Ÿæˆç»™ AI çš„ Prompt ç‰‡æ®µ
                context = (
                    f"ã€ç”¨æˆ·æ ¸å¿ƒæŒä»“ï¼ˆä¸­å›½Aè‚¡ï¼‰ã€‘ä»£ç åˆ—è¡¨: {', '.join(a_share_codes)}ã€‚\n"
                    f"æŒ‡ä»¤ï¼šè¯·åŠ¡å¿…æ ¹æ®ä»£ç ï¼ˆå¦‚ 600406 -> å›½ç”µå—ç‘ï¼‰è¯†åˆ«å¯¹åº”çš„å…¬å¸å®ä½“åŠæ‰€å±è¡Œä¸šäº§ä¸šé“¾ï¼Œ"
                    f"è‹¥æ–°é—»æ¶‰åŠè¿™äº›å…¬å¸æˆ–å…¶ä¸Šä¸‹æ¸¸ï¼Œè¯·æ ‡è®°ä¸ºã€ğŸ”´ æŒä»“å…³è”ã€‘ã€‚"
                )
                print(f"[Context] æˆåŠŸåŠ è½½ {len(a_share_codes)} åªæŒä»“è‚¡ç¥¨ä¸Šä¸‹æ–‡")
                return context
            else:
                print(f"[Context] è·å–æŒä»“å¤±è´¥: HTTP {response.status_code}")
                return ""
        except Exception as e:
            print(f"[Context] åŒæ­¥æŒä»“å‡ºé”™: {e}")
            return ""

    def _export_json_for_stock_analysis(self, ai_result: AIAnalysisResult) -> None:
        """å°† AI åˆ†æç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶"""
        try:
            output_dir = Path("output")
            output_dir.mkdir(parents=True, exist_ok=True)
            file_path = output_dir / "news_summary.json"

            # ä¼˜å…ˆä½¿ç”¨ AI ç”Ÿæˆçš„ç»“æ„åŒ–æ•°æ® (stock_analysis_data)
            if ai_result.stock_analysis_data and len(ai_result.stock_analysis_data) > 0:
                data = ai_result.stock_analysis_data
                print(f"[å¯¼å‡º] æˆåŠŸæå– {len(data)} æ¡è¡Œä¸šåˆ†ç±»æ•°æ®")
            else:
                # å…œåº•ï¼šå¦‚æœæ²¡æœ‰ç»“æ„åŒ–æ•°æ®ï¼Œå°±æŠŠæ•´æ®µæ–‡æœ¬ç®—ä½œ Macro
                print("[å¯¼å‡º] è­¦å‘Šï¼šAI æœªè¿”å›ç»“æ„åŒ–æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å…œåº•")
                data = [{
                    "category": "Macro",
                    "title": "å¸‚åœºå®è§‚ç»¼è¿°",
                    "summary": ai_result.core_trends[:200] + "..." if ai_result.core_trends else "æ— åˆ†æå†…å®¹",
                    "sentiment": "Neutral"
                }]

            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"[å¯¼å‡º] Stock Analysis ä¸“ç”¨æ•°æ®å·²ä¿å­˜: {file_path}")
        except Exception as e:
            print(f"[å¯¼å‡º] ä¿å­˜ JSON å¤±è´¥: {e}")

    def _setup_proxy(self) -> None:
        """è®¾ç½®ä»£ç†é…ç½®"""
        if not self.is_github_actions and self.ctx.config["USE_PROXY"]:
            self.proxy_url = self.ctx.config["DEFAULT_PROXY"]
            print("æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†")
        elif not self.is_github_actions and not self.ctx.config["USE_PROXY"]:
            print("æœ¬åœ°ç¯å¢ƒï¼Œæœªå¯ç”¨ä»£ç†")
        else:
            print("GitHub Actionsç¯å¢ƒï¼Œä¸ä½¿ç”¨ä»£ç†")

    def _check_version_update(self) -> None:
        """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
        try:
            need_update, remote_version = check_version_update(
                __version__, self.ctx.config["VERSION_CHECK_URL"], self.proxy_url
            )

            if need_update and remote_version:
                self.update_info = {
                    "current_version": __version__,
                    "remote_version": remote_version,
                }
                print(f"å‘ç°æ–°ç‰ˆæœ¬: {remote_version} (å½“å‰: {__version__})")
            else:
                print("ç‰ˆæœ¬æ£€æŸ¥å®Œæˆï¼Œå½“å‰ä¸ºæœ€æ–°ç‰ˆæœ¬")
        except Exception as e:
            print(f"ç‰ˆæœ¬æ£€æŸ¥å‡ºé”™: {e}")

    def _get_mode_strategy(self) -> Dict:
        """è·å–å½“å‰æ¨¡å¼çš„ç­–ç•¥é…ç½®"""
        return self.MODE_STRATEGIES.get(self.report_mode, self.MODE_STRATEGIES["daily"])

    def _has_notification_configured(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦é…ç½®äº†ä»»ä½•é€šçŸ¥æ¸ é“"""
        cfg = self.ctx.config
        return any(
            [
                cfg["FEISHU_WEBHOOK_URL"],
                cfg["DINGTALK_WEBHOOK_URL"],
                cfg["WEWORK_WEBHOOK_URL"],
                (cfg["TELEGRAM_BOT_TOKEN"] and cfg["TELEGRAM_CHAT_ID"]),
                (
                    cfg["EMAIL_FROM"]
                    and cfg["EMAIL_PASSWORD"]
                    and cfg["EMAIL_TO"]
                ),
                (cfg["NTFY_SERVER_URL"] and cfg["NTFY_TOPIC"]),
                cfg["BARK_URL"],
                cfg["SLACK_WEBHOOK_URL"],
                cfg["GENERIC_WEBHOOK_URL"],
            ]
        )

    def _has_valid_content(
        self, stats: List[Dict], new_titles: Optional[Dict] = None
    ) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ–°é—»å†…å®¹"""
        if self.report_mode == "incremental":
            # å¢é‡æ¨¡å¼ï¼šå¿…é¡»æœ‰æ–°å¢æ ‡é¢˜ä¸”åŒ¹é…äº†å…³é”®è¯æ‰æ¨é€
            has_new_titles = bool(
                new_titles and any(len(titles) > 0 for titles in new_titles.values())
            )
            has_matched_news = any(stat["count"] > 0 for stat in stats)
            return has_new_titles and has_matched_news
        elif self.report_mode == "current":
            # currentæ¨¡å¼ï¼šåªè¦statsæœ‰å†…å®¹å°±è¯´æ˜æœ‰åŒ¹é…çš„æ–°é—»
            return any(stat["count"] > 0 for stat in stats)
        else:
            # å½“æ—¥æ±‡æ€»æ¨¡å¼ä¸‹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„é¢‘ç‡è¯æ–°é—»æˆ–æ–°å¢æ–°é—»
            has_matched_news = any(stat["count"] > 0 for stat in stats)
            has_new_news = bool(
                new_titles and any(len(titles) > 0 for titles in new_titles.values())
            )
            return has_matched_news or has_new_news

    def _run_ai_analysis(
        self,
        stats: List[Dict],
        rss_items: Optional[List[Dict]],
        mode: str,
        report_type: str,
        id_to_name: Optional[Dict],
    ) -> Optional[AIAnalysisResult]:
        """æ‰§è¡Œ AI åˆ†æ"""
        analysis_config = self.ctx.config.get("AI_ANALYSIS", {})
        if not analysis_config.get("ENABLED", False):
            return None

        print("[AI] æ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
        try:
            # 1. è·å–æŒä»“ä¸Šä¸‹æ–‡ (æ–°å¢é€»è¾‘)
            portfolio_context = self._fetch_portfolio_context()

            ai_config = self.ctx.config.get("AI", {})
            debug_mode = self.ctx.config.get("DEBUG", False)
            analyzer = AIAnalyzer(ai_config, analysis_config, self.ctx.get_time, debug=debug_mode)

            # æå–å¹³å°åˆ—è¡¨
            platforms = list(id_to_name.values()) if id_to_name else []

            # æå–å…³é”®è¯åˆ—è¡¨
            keywords = [s.get("word", "") for s in stats if s.get("word")] if stats else []

            # 2. ä¼ é€’ portfolio_context ç»™åˆ†æå™¨
            result = analyzer.analyze(
                stats=stats,
                rss_stats=rss_items,
                report_mode=mode,
                report_type=report_type,
                platforms=platforms,
                keywords=keywords,
                portfolio_context=portfolio_context  # ğŸ‘ˆ å…³é”®æ³¨å…¥ç‚¹
            )

            if result.success:
                if result.error:
                    # æˆåŠŸä½†æœ‰è­¦å‘Šï¼ˆå¦‚ JSON è§£æé—®é¢˜ä½†ä½¿ç”¨äº†åŸå§‹æ–‡æœ¬ï¼‰
                    print(f"[AI] åˆ†æå®Œæˆï¼ˆæœ‰è­¦å‘Š: {result.error}ï¼‰")
                else:
                    print("[AI] åˆ†æå®Œæˆ")
            else:
                print(f"[AI] åˆ†æå¤±è´¥: {result.error}")

            return result
        except Exception as e:
            import traceback
            error_type = type(e).__name__
            error_msg = str(e)
            # æˆªæ–­è¿‡é•¿çš„é”™è¯¯æ¶ˆæ¯
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."
            print(f"[AI] åˆ†æå‡ºé”™ ({error_type}): {error_msg}")
            # è¯¦ç»†é”™è¯¯æ—¥å¿—åˆ° stderr
            import sys
            print(f"[AI] è¯¦ç»†é”™è¯¯å †æ ˆ:", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            return AIAnalysisResult(success=False, error=f"{error_type}: {error_msg}")

    def _load_analysis_data(
        self,
        quiet: bool = False,
    ) -> Optional[Tuple[Dict, Dict, Dict, Dict, List, List]]:
        """ç»Ÿä¸€çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼Œä½¿ç”¨å½“å‰ç›‘æ§å¹³å°åˆ—è¡¨è¿‡æ»¤å†å²æ•°æ®"""
        try:
            # è·å–å½“å‰é…ç½®çš„ç›‘æ§å¹³å°IDåˆ—è¡¨
            current_platform_ids = self.ctx.platform_ids
            if not quiet:
                print(f"å½“å‰ç›‘æ§å¹³å°: {current_platform_ids}")

            all_results, id_to_name, title_info = self.ctx.read_today_titles(
                current_platform_ids, quiet=quiet
            )

            if not all_results:
                print("æ²¡æœ‰æ‰¾åˆ°å½“å¤©çš„æ•°æ®")
                return None

            total_titles = sum(len(titles) for titles in all_results.values())
            if not quiet:
                print(f"è¯»å–åˆ° {total_titles} ä¸ªæ ‡é¢˜ï¼ˆå·²æŒ‰å½“å‰ç›‘æ§å¹³å°è¿‡æ»¤ï¼‰")

            new_titles = self.ctx.detect_new_titles(current_platform_ids, quiet=quiet)
            word_groups, filter_words, global_filters = self.ctx.load_frequency_words()

            return (
                all_results,
                id_to_name,
                title_info,
                new_titles,
                word_groups,
                filter_words,
                global_filters,
            )
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None

    def _prepare_current_title_info(self, results: Dict, time_info: str) -> Dict:
        """ä»å½“å‰æŠ“å–ç»“æœæ„å»ºæ ‡é¢˜ä¿¡æ¯"""
        title_info = {}
        for source_id, titles_data in results.items():
            title_info[source_id] = {}
            for title, title_data in titles_data.items():
                ranks = title_data.get("ranks", [])
                url = title_data.get("url", "")
                mobile_url = title_data.get("mobileUrl", "")

                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
        return title_info

    def _prepare_standalone_data(
        self,
        results: Dict,
        id_to_name: Dict,
        title_info: Optional[Dict] = None,
        rss_items: Optional[List[Dict]] = None,
    ) -> Optional[Dict]:
        """
        ä»åŸå§‹æ•°æ®ä¸­æå–ç‹¬ç«‹å±•ç¤ºåŒºæ•°æ®
        """
        display_config = self.ctx.config.get("DISPLAY", {})
        regions = display_config.get("REGIONS", {})
        standalone_config = display_config.get("STANDALONE", {})

        if not regions.get("STANDALONE", False):
            return None

        platform_ids = standalone_config.get("PLATFORMS", [])
        rss_feed_ids = standalone_config.get("RSS_FEEDS", [])
        max_items = standalone_config.get("MAX_ITEMS", 20)

        if not platform_ids and not rss_feed_ids:
            return None

        standalone_data = {
            "platforms": [],
            "rss_feeds": [],
        }

        # æ‰¾å‡ºæœ€æ–°æ‰¹æ¬¡æ—¶é—´
        latest_time = None
        if title_info:
            for source_titles in title_info.values():
                for title_data in source_titles.values():
                    last_time = title_data.get("last_time", "")
                    if last_time:
                        if latest_time is None or last_time > latest_time:
                            latest_time = last_time

        # æå–çƒ­æ¦œå¹³å°æ•°æ®
        for platform_id in platform_ids:
            if platform_id not in results:
                continue

            platform_name = id_to_name.get(platform_id, platform_id)
            platform_titles = results[platform_id]

            items = []
            for title, title_data in platform_titles.items():
                meta = {}
                if title_info and platform_id in title_info and title in title_info[platform_id]:
                    meta = title_info[platform_id][title]

                if latest_time and meta:
                    if meta.get("last_time") != latest_time:
                        continue

                current_ranks = title_data.get("ranks", [])
                current_rank = current_ranks[-1] if current_ranks else 0

                historical_ranks = meta.get("ranks", []) if meta else []
                all_ranks = historical_ranks.copy()
                for rank in current_ranks:
                    if rank not in all_ranks:
                        all_ranks.append(rank)
                display_ranks = all_ranks if all_ranks else current_ranks

                item = {
                    "title": title,
                    "url": title_data.get("url", ""),
                    "mobileUrl": title_data.get("mobileUrl", ""),
                    "rank": current_rank,
                    "ranks": display_ranks,
                    "first_time": meta.get("first_time", ""),
                    "last_time": meta.get("last_time", ""),
                    "count": meta.get("count", 1),
                }
                items.append(item)

            items.sort(key=lambda x: x["rank"] if x["rank"] > 0 else 9999)

            if max_items > 0:
                items = items[:max_items]

            if items:
                standalone_data["platforms"].append({
                    "id": platform_id,
                    "name": platform_name,
                    "items": items,
                })

        # æå– RSS æ•°æ®
        if rss_items and rss_feed_ids:
            feed_items_map = {}
            for item in rss_items:
                feed_id = item.get("feed_id", "")
                if feed_id in rss_feed_ids:
                    if feed_id not in feed_items_map:
                        feed_items_map[feed_id] = {
                            "name": item.get("feed_name", feed_id),
                            "items": [],
                        }
                    feed_items_map[feed_id]["items"].append({
                        "title": item.get("title", ""),
                        "url": item.get("url", ""),
                        "published_at": item.get("published_at", ""),
                        "author": item.get("author", ""),
                    })

            for feed_id in rss_feed_ids:
                if feed_id in feed_items_map:
                    feed_data = feed_items_map[feed_id]
                    items = feed_data["items"]
                    if max_items > 0:
                        items = items[:max_items]
                    if items:
                        standalone_data["rss_feeds"].append({
                            "id": feed_id,
                            "name": feed_data["name"],
                            "items": items,
                        })

        if not standalone_data["platforms"] and not standalone_data["rss_feeds"]:
            return None

        return standalone_data

    def _deduplicate_items(self, items_list: List[Dict]) -> List[Dict]:
        """
        æ ¸å¿ƒå»é‡é€»è¾‘ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦ > 0.7 çš„è§†ä¸ºåŒä¸€æ¡æ–°é—»ï¼Œåˆå¹¶é¢‘æ¬¡ã€‚
        åŒæ—¶æ‰§è¡Œ max_news_per_keyword æˆªæ–­ã€‚
        """
        from difflib import SequenceMatcher
        if not items_list:
            return []

        deduped = []
        for item in items_list:
            found = False
            title = item.get('title', '')
            
            for exist in deduped:
                # è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥
                similarity = SequenceMatcher(None, title, exist['title']).ratio()
                if similarity > 0.7:
                    # åˆå¹¶é€»è¾‘ï¼šå¢åŠ è®¡æ•°ï¼Œåˆå¹¶æ’å
                    exist['count'] = exist.get('count', 1) + 1
                    if 'ranks' in item and 'ranks' in exist:
                        # åˆå¹¶å»é‡æ’å
                        combined_ranks = list(set(exist['ranks'] + item['ranks']))
                        exist['ranks'] = sorted(combined_ranks)
                    found = True
                    break
            
            if not found:
                # å¦‚æœæ²¡æ‰¾åˆ°ç›¸ä¼¼çš„ï¼Œè®¾ç½®åˆå§‹ count ä¸º 1
                if 'count' not in item:
                    item['count'] = 1
                deduped.append(item)
        
        # === æ–°å¢ï¼šå¼ºåˆ¶æ‰§è¡Œæ•°é‡é™åˆ¶ ===
        # è¯»å–é…ç½®ä¸­çš„é™åˆ¶ï¼Œé»˜è®¤ 3 æ¡
        limit = self.ctx.config.get("REPORT", {}).get("MAX_NEWS_PER_KEYWORD", 3)
        if limit > 0 and len(deduped) > limit:
            deduped = deduped[:limit]
            
        return deduped

    def _run_analysis_pipeline(
        self,
        data_source: Dict,
        mode: str,
        title_info: Dict,
        new_titles: Dict,
        word_groups: List[Dict],
        filter_words: List[str],
        id_to_name: Dict,
        failed_ids: Optional[List] = None,
        global_filters: Optional[List[str]] = None,
        quiet: bool = False,
        rss_items: Optional[List[Dict]] = None,
        rss_new_items: Optional[List[Dict]] = None,
        standalone_data: Optional[Dict] = None,
    ) -> Tuple[List[Dict], Optional[str], Optional[AIAnalysisResult]]:
        """ç»Ÿä¸€çš„åˆ†ææµæ°´çº¿"""

        # ç»Ÿè®¡è®¡ç®—
        stats, total_titles = self.ctx.count_frequency(
            data_source,
            word_groups,
            filter_words,
            id_to_name,
            title_info,
            new_titles,
            mode=mode,
            global_filters=global_filters,
            quiet=quiet,
        )

        # === æ ¸å¿ƒä¼˜åŒ–ï¼šå¯¹ç»Ÿè®¡åçš„ titles è¿›è¡Œè¯­ä¹‰å»é‡ ===
        if stats:
            for group in stats:
                if 'titles' in group and group['titles']:
                    original_len = len(group['titles'])
                    group['titles'] = self._deduplicate_items(group['titles'])
                    new_len = len(group['titles'])
                    if not quiet and original_len != new_len:
                        print(f"[ä¼˜åŒ–] å…³é”®è¯ '{group.get('word')}' ä¸‹æ ‡é¢˜å»é‡: {original_len} -> {new_len}")

        # å¦‚æœæ˜¯ platform æ¨¡å¼ï¼Œè½¬æ¢æ•°æ®ç»“æ„
        if self.ctx.display_mode == "platform" and stats:
            stats = convert_keyword_stats_to_platform_stats(
                stats,
                self.ctx.weight_config,
                self.ctx.rank_threshold,
            )

        # AI åˆ†æ
        ai_result = None
        ai_config = self.ctx.config.get("AI_ANALYSIS", {})
        if ai_config.get("ENABLED", False) and stats:
            mode_strategy = self._get_mode_strategy()
            report_type = mode_strategy["report_type"]
            ai_result = self._run_ai_analysis(
                stats, rss_items, mode, report_type, id_to_name
            )

        # HTMLç”Ÿæˆ
        html_file = None
        if self.ctx.config["STORAGE"]["FORMATS"]["HTML"]:
            html_file = self.ctx.generate_html(
                stats,
                total_titles,
                failed_ids=failed_ids,
                new_titles=new_titles,
                id_to_name=id_to_name,
                mode=mode,
                update_info=self.update_info if self.ctx.config["SHOW_VERSION_UPDATE"] else None,
                rss_items=rss_items,
                rss_new_items=rss_new_items,
                ai_analysis=ai_result,
                standalone_data=standalone_data,
            )

        return stats, html_file, ai_result

    def _send_notification_if_needed(
        self,
        stats: List[Dict],
        report_type: str,
        mode: str,
        failed_ids: Optional[List] = None,
        new_titles: Optional[Dict] = None,
        id_to_name: Optional[Dict] = None,
        html_file_path: Optional[str] = None,
        rss_items: Optional[List[Dict]] = None,
        rss_new_items: Optional[List[Dict]] = None,
        standalone_data: Optional[Dict] = None,
        ai_result: Optional[AIAnalysisResult] = None,
    ) -> bool:
        """ç»Ÿä¸€çš„é€šçŸ¥å‘é€é€»è¾‘"""
        has_notification = self._has_notification_configured()
        cfg = self.ctx.config

        # ---------------------------------------------------------------
        # 1. AI åˆ†æï¼šå¿…é¡»ä½¿ç”¨ã€å®Œæ•´æ•°æ®ã€‘(åŒ…å«å¼±ä¿¡å·)
        # ---------------------------------------------------------------
        if ai_result is None:
            ai_config = cfg.get("AI_ANALYSIS", {})
            if ai_config.get("ENABLED", False):
                # è¿™é‡Œä¼ å…¥çš„æ˜¯ stats å…¨é‡æ•°æ®ï¼Œç¡®ä¿ AI èƒ½çœ‹åˆ°æ’åé åçš„å¼±ä¿¡å·
                ai_result = self._run_analysis_pipeline(
                    stats, rss_items, mode, report_type, id_to_name
                )

        # ---------------------------------------------------------------
        # 2. åˆ—è¡¨å±•ç¤ºï¼šæ„å»ºã€ç²¾ç®€æ•°æ®ã€‘(éšè—å¼±ä¿¡å·)
        # ---------------------------------------------------------------
        filtered_stats_for_display = []
        if stats:
            for group in stats:
                titles = group.get('titles', [])
                if not titles:
                    continue
                
                is_strong = len(titles) >= 2
                if not is_strong:
                    # æ£€æŸ¥æ’åï¼Œå¦‚æœæ²¡æœ‰æ’åæ•°æ®(rank=0)ï¼Œé»˜è®¤è§†ä¸ºå¼±
                    top_rank = titles[0].get('rank', 999)
                    if top_rank > 0 and top_rank <= 10:
                        is_strong = True
                
                if is_strong:
                    filtered_stats_for_display.append(group)

        # ---------------------------------------------------------------
        # 3. ç”ŸæˆæŠ¥å‘Šå¹¶æ¨é€
        # ---------------------------------------------------------------
        report_data = self.ctx.prepare_report(filtered_stats_for_display, failed_ids, new_titles, id_to_name, mode)
        
        update_info_to_send = self.update_info if cfg["SHOW_VERSION_UPDATE"] else None

        has_news_content = self._has_valid_content(filtered_stats_for_display, new_titles)
        has_rss_content = bool(rss_items and len(rss_items) > 0)
        has_any_content = has_news_content or has_rss_content

        if cfg["ENABLE_NOTIFICATION"] and has_notification and has_any_content:
            # è¾“å‡ºæ¨é€å†…å®¹ç»Ÿè®¡
            news_count = sum(len(stat.get("titles", [])) for stat in filtered_stats_for_display)
            rss_count = sum(stat.get("count", 0) for stat in rss_items) if rss_items else 0
            
            content_parts = []
            if news_count > 0:
                content_parts.append(f"çƒ­æ¦œ {news_count} æ¡")
            if rss_count > 0:
                content_parts.append(f"RSS {rss_count} æ¡")
            total_count = news_count + rss_count
            print(f"[æ¨é€] å‡†å¤‡å‘é€ï¼š{' + '.join(content_parts)}ï¼Œåˆè®¡ {total_count} æ¡")

            # æ¨é€çª—å£æ§åˆ¶
            if cfg["PUSH_WINDOW"]["ENABLED"]:
                push_manager = self.ctx.create_push_manager()
                time_range_start = cfg["PUSH_WINDOW"]["TIME_RANGE"]["START"]
                time_range_end = cfg["PUSH_WINDOW"]["TIME_RANGE"]["END"]

                if not push_manager.is_in_time_range(time_range_start, time_range_end):
                    now = self.ctx.get_time()
                    print(f"æ¨é€çª—å£æ§åˆ¶ï¼šå½“å‰æ—¶é—´ {now.strftime('%H:%M')} ä¸åœ¨æ¨é€æ—¶é—´çª—å£ {time_range_start}-{time_range_end} å†…ï¼Œè·³è¿‡æ¨é€")
                    return False

                if cfg["PUSH_WINDOW"]["ONCE_PER_DAY"]:
                    if push_manager.has_pushed_today():
                        print(f"æ¨é€çª—å£æ§åˆ¶ï¼šä»Šå¤©å·²æ¨é€è¿‡ï¼Œè·³è¿‡æœ¬æ¬¡æ¨é€")
                        return False

            dispatcher = self.ctx.create_notification_dispatcher()
            results = dispatcher.dispatch_all(
                report_data=report_data,
                report_type=report_type,
                update_info=update_info_to_send,
                proxy_url=self.proxy_url,
                mode=mode,
                html_file_path=html_file_path,
                rss_items=rss_items,
                rss_new_items=rss_new_items,
                ai_analysis=ai_result, 
                standalone_data=standalone_data,
            )

            if results and any(results.values()) and cfg["PUSH_WINDOW"]["ENABLED"] and cfg["PUSH_WINDOW"]["ONCE_PER_DAY"]:
                push_manager = self.ctx.create_push_manager()
                push_manager.record_push(report_type)

            return True
        elif cfg["ENABLE_NOTIFICATION"] and has_notification and not has_any_content:
             print(f"è·³è¿‡{report_type}é€šçŸ¥ï¼šç»ç­›é€‰åæœªæ£€æµ‹åˆ°åŒ¹é…çš„å¼ºä¿¡å·æ–°é—»")

        return False

    def _initialize_and_check_config(self) -> None:
        """é€šç”¨åˆå§‹åŒ–å’Œé…ç½®æ£€æŸ¥"""
        now = self.ctx.get_time()
        print(f"å½“å‰åŒ—äº¬æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}")

        if not self.ctx.config["ENABLE_CRAWLER"]:
            print("çˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨ï¼Œç¨‹åºé€€å‡º")
            return

        has_notification = self._has_notification_configured()
        if not self.ctx.config["ENABLE_NOTIFICATION"]:
            print("é€šçŸ¥åŠŸèƒ½å·²ç¦ç”¨ï¼Œå°†åªè¿›è¡Œæ•°æ®æŠ“å–")
        elif not has_notification:
            print("æœªé…ç½®ä»»ä½•é€šçŸ¥æ¸ é“ï¼Œå°†åªè¿›è¡Œæ•°æ®æŠ“å–ï¼Œä¸å‘é€é€šçŸ¥")

        mode_strategy = self._get_mode_strategy()
        print(f"æŠ¥å‘Šæ¨¡å¼: {self.report_mode}")
        print(f"è¿è¡Œæ¨¡å¼: {mode_strategy['description']}")

    def _crawl_data(self) -> Tuple[Dict, Dict, List]:
        """æ‰§è¡Œæ•°æ®çˆ¬å–"""
        ids = []
        for platform in self.ctx.platforms:
            if "name" in platform:
                ids.append((platform["id"], platform["name"]))
            else:
                ids.append(platform["id"])

        print(f"å¼€å§‹çˆ¬å–æ•°æ®ï¼Œç›‘æ§å¹³å°: {[p.get('name', p['id']) for p in self.ctx.platforms]}")
        Path("output").mkdir(parents=True, exist_ok=True)

        results, id_to_name, failed_ids = self.data_fetcher.crawl_websites(
            ids, self.request_interval
        )

        crawl_time = self.ctx.format_time()
        crawl_date = self.ctx.format_date()
        news_data = convert_crawl_results_to_news_data(
            results, id_to_name, failed_ids, crawl_time, crawl_date
        )

        if self.storage_manager.save_news_data(news_data):
            print(f"æ•°æ®å·²ä¿å­˜åˆ°å­˜å‚¨åç«¯: {self.storage_manager.backend_name}")

        if self.ctx.config["STORAGE"]["FORMATS"]["TXT"]:
            self.ctx.save_titles(results, id_to_name, failed_ids)

        return results, id_to_name, failed_ids

    def _crawl_rss_data(self) -> Tuple[Optional[List[Dict]], Optional[List[Dict]], Optional[List[Dict]]]:
        """æ‰§è¡Œ RSS æ•°æ®æŠ“å–"""
        if not self.ctx.rss_enabled:
            return None, None, None

        rss_feeds = self.ctx.rss_feeds
        if not rss_feeds:
            return None, None, None

        try:
            from trendradar.crawler.rss import RSSFetcher, RSSFeedConfig
            feeds = []
            for feed_config in rss_feeds:
                max_age_days = None
                if feed_config.get("max_age_days") is not None:
                    max_age_days = int(feed_config["max_age_days"])

                feed = RSSFeedConfig(
                    id=feed_config.get("id", ""),
                    name=feed_config.get("name", ""),
                    url=feed_config.get("url", ""),
                    max_items=feed_config.get("max_items", 50),
                    enabled=feed_config.get("enabled", True),
                    max_age_days=max_age_days,
                )
                if feed.id and feed.url and feed.enabled:
                    feeds.append(feed)

            if not feeds:
                return None, None, None

            rss_config = self.ctx.rss_config
            rss_proxy_url = rss_config.get("PROXY_URL", "") or self.proxy_url or ""
            fetcher = RSSFetcher(
                feeds=feeds,
                request_interval=rss_config.get("REQUEST_INTERVAL", 2000),
                timeout=rss_config.get("TIMEOUT", 15),
                use_proxy=rss_config.get("USE_PROXY", False),
                proxy_url=rss_proxy_url,
                timezone=self.ctx.config.get("TIMEZONE", "Asia/Shanghai"),
                freshness_enabled=rss_config.get("FRESHNESS_FILTER", {}).get("ENABLED", True),
                default_max_age_days=rss_config.get("FRESHNESS_FILTER", {}).get("MAX_AGE_DAYS", 3),
            )

            rss_data = fetcher.fetch_all()
            if self.storage_manager.save_rss_data(rss_data):
                return self._process_rss_data_by_mode(rss_data)
            return None, None, None

        except Exception as e:
            print(f"[RSS] æŠ“å–å¤±è´¥: {e}")
            return None, None, None

    def _process_rss_data_by_mode(self, rss_data) -> Tuple[Optional[List[Dict]], Optional[List[Dict]], Optional[List[Dict]]]:
        """å¤„ç† RSS æ•°æ®ï¼ˆæŒ‰æ¨¡å¼è¿‡æ»¤ï¼‰"""
        from trendradar.core.analyzer import count_rss_frequency
        rss_display_enabled = self.ctx.config.get("DISPLAY", {}).get("REGIONS", {}).get("RSS", True)

        try:
            word_groups, filter_words, global_filters = self.ctx.load_frequency_words()
        except FileNotFoundError:
            word_groups, filter_words, global_filters = [], [], []

        raw_rss_items = None
        new_items_dict = self.storage_manager.detect_new_rss_items(rss_data)
        new_items_list = self._convert_rss_items_to_list(new_items_dict, rss_data.id_to_name) if new_items_dict else None

        if self.report_mode == "incremental":
            raw_rss_items = new_items_list
        elif self.report_mode == "current":
            latest_data = self.storage_manager.get_latest_rss_data(rss_data.date)
            if latest_data:
                raw_rss_items = self._convert_rss_items_to_list(latest_data.items, latest_data.id_to_name)
        else:
            all_data = self.storage_manager.get_rss_data(rss_data.date)
            if all_data:
                raw_rss_items = self._convert_rss_items_to_list(all_data.items, all_data.id_to_name)

        if not rss_display_enabled:
            return None, None, raw_rss_items

        rss_stats = None
        rss_new_stats = None
        if raw_rss_items:
            rss_stats, _ = count_rss_frequency(
                rss_items=raw_rss_items,
                word_groups=word_groups,
                filter_words=filter_words,
                global_filters=global_filters,
                new_items=new_items_list,
                max_news_per_keyword=self.ctx.config.get("MAX_NEWS_PER_KEYWORD", 0),
                sort_by_position_first=self.ctx.config.get("SORT_BY_POSITION_FIRST", False),
                timezone=self.ctx.timezone,
                rank_threshold=self.rank_threshold,
                quiet=False,
            )
            if rss_stats:
                for group in rss_stats:
                    if 'titles' in group:
                        group['titles'] = self._deduplicate_items(group['titles'])

        return rss_stats, rss_new_stats, raw_rss_items

    def _convert_rss_items_to_list(self, items_dict: Dict, id_to_name: Dict) -> List[Dict]:
        """å°† RSS æ¡ç›®å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼"""
        rss_items = []
        rss_config = self.ctx.rss_config
        freshness_enabled = rss_config.get("FRESHNESS_FILTER", {}).get("ENABLED", True)
        default_max_age_days = rss_config.get("FRESHNESS_FILTER", {}).get("MAX_AGE_DAYS", 3)
        timezone = self.ctx.config.get("TIMEZONE", "Asia/Shanghai")

        for feed_id, items in items_dict.items():
            for item in items:
                if freshness_enabled and item.published_at:
                    if not is_within_days(item.published_at, default_max_age_days, timezone):
                        continue
                rss_items.append({
                    "title": item.title,
                    "feed_id": feed_id,
                    "feed_name": id_to_name.get(feed_id, feed_id),
                    "url": item.url,
                    "published_at": item.published_at,
                    "summary": item.summary,
                    "author": item.author,
                })
        return rss_items

    def _execute_mode_strategy(
        self, mode_strategy: Dict, results: Dict, id_to_name: Dict, failed_ids: List,
        rss_items: Optional[List[Dict]] = None,
        rss_new_items: Optional[List[Dict]] = None,
        raw_rss_items: Optional[List[Dict]] = None,
    ) -> Optional[str]:
        """æ‰§è¡Œæ¨¡å¼ç‰¹å®šé€»è¾‘"""
        current_platform_ids = self.ctx.platform_ids
        word_groups, filter_words, global_filters = self.ctx.load_frequency_words()

        html_file = None
        stats = []
        ai_result = None
        title_info = None
        new_titles = self.ctx.detect_new_titles(current_platform_ids)

        # åŠ è½½å†å²/å½“æ—¥å…¨é‡æ•°æ®
        analysis_data = self._load_analysis_data()
        if analysis_data:
            (all_results, h_id_to_name, h_title_info, h_new_titles, _, _, _) = analysis_data
            
            # åˆå¹¶ ID æ˜ å°„
            combined_id_to_name = {**h_id_to_name, **id_to_name}
            
            # å¤„ç†ç‹¬ç«‹å±•ç¤ºåŒºæ•°æ®
            standalone_data = self._prepare_standalone_data(
                all_results if self.report_mode in ["current", "daily"] else results,
                combined_id_to_name,
                h_title_info if self.report_mode in ["current", "daily"] else None,
                raw_rss_items
            )

            # è°ƒç”¨åˆ†ææµæ°´çº¿
            stats, html_file, ai_result = self._run_analysis_pipeline(
                all_results if self.report_mode in ["current", "daily"] else results,
                self.report_mode,
                h_title_info if self.report_mode in ["current", "daily"] else self._prepare_current_title_info(results, self.ctx.format_time()),
                h_new_titles if self.report_mode in ["current", "daily"] else new_titles,
                word_groups,
                filter_words,
                combined_id_to_name,
                failed_ids=failed_ids,
                global_filters=global_filters,
                rss_items=rss_items,
                rss_new_items=rss_new_items,
                standalone_data=standalone_data,
            )
            
            # === JSON å¯¼å‡ºå…œåº•é€»è¾‘ ===
            if ai_result: 
                self._export_json_for_stock_analysis(ai_result)
            else:
                print("[ç³»ç»Ÿ] æœªè§¦å‘ AI åˆ†æï¼Œç”Ÿæˆå…œåº• JSON æ–‡ä»¶...")
                dummy_result = AIAnalysisResult(success=True, core_trends="æ— é‡å¤§å¸‚åœºå¼‚åŠ¨")
                self._export_json_for_stock_analysis(dummy_result)
            
            # === [ä¿®å¤] å°è¯•ä¿å­˜ AI åˆ†æç»“æœåˆ°è¿œç¨‹å­˜å‚¨ ===
            # ä½¿ç”¨æ­£ç¡®çš„ save_ai_result æ–¹æ³•ï¼Œè€Œä¸æ˜¯é”™è¯¯çš„ save_daily_trends
            if ai_result and hasattr(self.storage_manager, "save_ai_result"):
                try:
                    # å°† AI ç»“æœå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿å­˜å‚¨
                    ai_data_dict = {
                        "core_trends": ai_result.core_trends,
                        "industry_analysis": ai_result.stock_analysis_data, # æ˜ å°„åˆ° industry
                        "market_sentiment": getattr(ai_result, "sentiment", "Neutral"),
                        "raw_json": self._export_json_for_stock_analysis(ai_result) # å¤ç”¨å¯¼å‡ºé€»è¾‘
                    }
                    self.storage_manager.save_ai_result(self.ctx.format_date(), ai_data_dict)
                    print(f"[å­˜å‚¨] AI åˆ†æç»“æœå·²ä¿å­˜åˆ° {self.storage_manager.backend_name}")
                except Exception as e:
                    logger.warning(f"[å­˜å‚¨] ä¿å­˜ AI ç»“æœå¤±è´¥: {e}")
            
            # å‘é€é€šçŸ¥
            if mode_strategy["should_send_notification"]:
                self._send_notification_if_needed(
                    stats, mode_strategy["report_type"], self.report_mode,
                    failed_ids=failed_ids, new_titles=h_new_titles, id_to_name=combined_id_to_name,
                    html_file_path=html_file, rss_items=rss_items, rss_new_items=rss_new_items,
                    standalone_data=standalone_data, ai_result=ai_result,
                )

        if self._should_open_browser() and html_file:
            webbrowser.open("file://" + str(Path(html_file).resolve()))

        return html_file

    def run(self) -> None:
        """æ‰§è¡Œåˆ†ææµç¨‹"""
        try:
            self._initialize_and_check_config()
            mode_strategy = self._get_mode_strategy()
            results, id_to_name, failed_ids = self._crawl_data()
            rss_items, rss_new_items, raw_rss_items = self._crawl_rss_data()
            self._execute_mode_strategy(
                mode_strategy, results, id_to_name, failed_ids,
                rss_items=rss_items, rss_new_items=rss_new_items,
                raw_rss_items=raw_rss_items
            )
        except Exception as e:
            print(f"æµç¨‹æ‰§è¡Œå‡ºé”™: {e}")
            if self.ctx.config.get("DEBUG", False): raise
        finally:
            self.ctx.cleanup()


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        analyzer = NewsAnalyzer()
        analyzer.run()
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")


if __name__ == "__main__":
    main()
