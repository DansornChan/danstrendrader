# coding=utf-8
"""
AI Clientï¼ˆç»ˆæå…¼å®¹ç‰ˆï¼‰

- å…¼å®¹æ—§ validate_config() â†’ (bool, str)
- LiteLLM æ­£ç¡® fallback
- é˜²æ­¢ model=list å¯¼è‡´ split å´©æºƒ
"""

import os
import logging
from typing import Any, Dict, List

from litellm import completion
from litellm.exceptions import (
    RateLimitError,
    BadRequestError,
    AuthenticationError,
)

logger = logging.getLogger(__name__)


class AIClient:
    def __init__(self, config: Dict[str, Any]):
        # ===== Primary =====
        self.model = config.get("MODEL") or os.getenv("PRIMARY_MODEL")
        self.api_key = config.get("API_KEY") or os.getenv("PRIMARY_API_KEY")

        # ===== Fallback =====
        self.fallback_models: List[Dict[str, str]] = config.get(
            "FALLBACK_MODELS", []
        )

        # ===== Params =====
        self.temperature = float(config.get("TEMPERATURE", 0.7))
        self.max_tokens = int(config.get("MAX_TOKENS", 5000))
        self.timeout = int(config.get("TIMEOUT", 120))
        self.num_retries = int(config.get("NUM_RETRIES", 2))

        self.dry_run = str(
            config.get("DRY_RUN_AI") or os.getenv("DRY_RUN_AI", "false")
        ).lower() == "true"

    # ------------------------------------------------------------------
    # âœ… æ—§ä»£ç å…¼å®¹æ¥å£ï¼ˆéå¸¸å…³é”®ï¼‰
    def validate_config(self):
        try:
            self._validate()
            return True, ""
        except Exception as e:
            return False, str(e)

    # ------------------------------------------------------------------
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        if self.dry_run:
            logger.warning("ğŸ§ª DRY_RUN_AI=trueï¼Œæœªè°ƒç”¨çœŸå®æ¨¡å‹")
            return self._dry_run_response(messages)

        params = {
            "model": self.model,      # âš ï¸ å¿…é¡»æ˜¯ string
            "messages": messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "timeout": self.timeout,
            "num_retries": self.num_retries,
        }

        if self.max_tokens > 0:
            params["max_tokens"] = self.max_tokens

        # âœ… LiteLLM å®˜æ–¹ fallback ç”¨æ³•
        if self.fallback_models:
            params["fallbacks"] = self.fallback_models

        try:
            logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
            resp = completion(**params)
            return resp.choices[0].message.content

        except (RateLimitError, BadRequestError) as e:
            logger.warning(
                f"âš ï¸ Primary å¤±è´¥ï¼Œå°†å°è¯• fallbackï¼ˆ{type(e).__name__}ï¼‰"
            )
            raise

        except AuthenticationError as e:
            logger.error(f"âŒ API Key é”™è¯¯: {e}")
            raise

    # ------------------------------------------------------------------
    def _dry_run_response(self, messages):
        preview = ""
        for m in messages:
            if m.get("role") == "user":
                preview += m.get("content", "")[:200]
        return f"ã€DRY RUNã€‘AI æœªè°ƒç”¨\næ‘˜è¦ï¼š{preview}"

    # ------------------------------------------------------------------
    def _validate(self):
        if not self.model:
            raise ValueError("æœªé…ç½® PRIMARY_MODEL")

        if not isinstance(self.model, str):
            raise ValueError("PRIMARY_MODEL å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        if "/" not in self.model:
            raise ValueError(
                f"æ¨¡å‹æ ¼å¼é”™è¯¯ï¼š{self.model}ï¼Œåº”ä¸º provider/model"
            )

        if not self.api_key:
            raise ValueError("æœªé…ç½® PRIMARY_API_KEY")

        if self.fallback_models:
            if not isinstance(self.fallback_models, list):
                raise ValueError("FALLBACK_MODELS å¿…é¡»æ˜¯ list")

            for fb in self.fallback_models:
                if not isinstance(fb, dict):
                    raise ValueError(f"éæ³• fallback é…ç½®: {fb}")
                if "model" not in fb or "api_key" not in fb:
                    raise ValueError(f"fallback ç¼ºå°‘å­—æ®µ: {fb}")