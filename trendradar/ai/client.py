# coding=utf-8
"""
AI Clientï¼ˆç¨³å®šå…¼å®¹ç‰ˆï¼‰

- LiteLLM Primary / Fallback æ­£ç¡®ç”¨æ³•
- å…¼å®¹æ—§ validate_config() è°ƒç”¨
- é˜²æ­¢ model è¢«é”™è¯¯ä¼ ä¸º list
"""

import os
import logging
from typing import Any, Dict, List

from litellm import completion
from litellm.exceptions import (
    RateLimitError,
    BadRequestError,
    AuthenticationError,
)

logger = logging.getLogger(__name__)


class AIClient:
    def __init__(self, config: Dict[str, Any]):
        # ===== Primary =====
        self.model: str = config.get("MODEL") or os.getenv("PRIMARY_MODEL")
        self.api_key: str = config.get("API_KEY") or os.getenv("PRIMARY_API_KEY")

        # ===== Fallbackï¼ˆä¸€å®šæ˜¯ listï¼‰=====
        self.fallback_models: List[Dict[str, str]] = config.get(
            "FALLBACK_MODELS", []
        )

        # ===== Params =====
        self.temperature = float(config.get("TEMPERATURE", 0.7))
        self.max_tokens = int(config.get("MAX_TOKENS", 5000))
        self.timeout = int(config.get("TIMEOUT", 120))
        self.num_retries = int(config.get("NUM_RETRIES", 2))

        self.dry_run = str(
            config.get("DRY_RUN_AI") or os.getenv("DRY_RUN_AI", "false")
        ).lower() == "true"

        self._validate()

    # ------------------------------------------------------------------

    # âœ… å…¼å®¹æ—§ä»£ç ï¼ˆä¸è¦åˆ ï¼‰
    def validate_config(self):
        self._validate()

    # ------------------------------------------------------------------

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        if self.dry_run:
            logger.warning("ğŸ§ª DRY_RUN_AI=trueï¼Œæœªè°ƒç”¨çœŸå®æ¨¡å‹")
            return self._dry_run_response(messages)

        params = {
            "model": self.model,               # âš ï¸ å¿…é¡»æ˜¯ string
            "messages": messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "timeout": self.timeout,
            "num_retries": self.num_retries,
        }

        if self.max_tokens > 0:
            params["max_tokens"] = self.max_tokens

        # âœ… LiteLLM æ­£ç¡® fallback æ–¹å¼
        if self.fallback_models:
            params["fallbacks"] = self.fallback_models

        try:
            logger.info(f"ğŸ¤– Primary æ¨¡å‹: {self.model}")
            resp = completion(**params)
            return resp.choices[0].message.content

        except (RateLimitError, BadRequestError) as e:
            logger.warning(
                f"âš ï¸ Primary å¤±è´¥ï¼Œé”™è¯¯={type(e).__name__}ï¼ŒLiteLLM å°†è‡ªåŠ¨å°è¯• fallback"
            )
            raise

        except AuthenticationError as e:
            logger.error(f"âŒ API Key é”™è¯¯: {e}")
            raise

    # ------------------------------------------------------------------

    def _dry_run_response(self, messages):
        preview = ""
        for m in messages:
            if m.get("role") == "user":
                preview += m.get("content", "")[:200]

        return (
            "ã€DRY RUNã€‘æœªè°ƒç”¨çœŸå®æ¨¡å‹\n\n"
            f"ç”¨æˆ·è¾“å…¥æ‘˜è¦ï¼š{preview}"
        )

    # ------------------------------------------------------------------

    def _validate(self):
        if not isinstance(self.model, str):
            raise ValueError(
                f"PRIMARY_MODEL å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œå½“å‰={self.model}"
            )

        if "/" not in self.model:
            raise ValueError(
                f"æ¨¡å‹æ ¼å¼é”™è¯¯ï¼š{self.model}ï¼Œåº”ä¸º provider/model"
            )

        if not self.api_key:
            raise ValueError("æœªé…ç½® PRIMARY_API_KEY")

        if self.fallback_models:
            if not isinstance(self.fallback_models, list):
                raise ValueError("FALLBACK_MODELS å¿…é¡»æ˜¯ list")

            for fb in self.fallback_models:
                if not isinstance(fb, dict):
                    raise ValueError(f"éæ³• fallback é…ç½®: {fb}")
                if "model" not in fb or "api_key" not in fb:
                    raise ValueError(f"fallback ç¼ºå°‘å­—æ®µ: {fb}")